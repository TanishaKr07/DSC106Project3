{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://githubtocolab.com/neurallatents/neurallatents.github.io/blob/master/notebooks/mc_maze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "XLjvP1NWcRGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MC_Maze Dataset\n",
        "\n",
        "[DANDI](https://dandiarchive.org/#/dandiset/000128)\n",
        "\n",
        "## 1 Overview\n",
        "\n",
        "The MC_Maze dataset includes data from four recording sessions of a macaque performing delayed center-out reaches, with neural activity recorded from the primary motor and dorsal premotor cortices. This data was provided by Krishna Shenoy, Mark Churchland, and Matt Kaufman from Stanford University, and you can learn more about the task design, data collection, and their analyses of the data in a number of papers, including [this](https://pubmed.ncbi.nlm.nih.gov/21040842/) (Churchland et al. 2010)."
      ],
      "metadata": {
        "id": "lMQptp8JcRGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "scrolled": true,
        "id": "RttekqdfcRGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Task\n",
        "\n",
        "The maze task is a delayed center-out reaching task, meaning that there was a period between target presentation and the go cue when the monkey could plan and prepare its movement. The reaches took place in a number of maze configurations, resulting in a variety of straight and curved reaches. In some trials, three targets were presented, though only one was reachable, further complicating the task for the monkey. The delayed reaching paradigm allows for the examination of neural activity during both movement preparation and execution."
      ],
      "metadata": {
        "id": "SZc_CqBWcRGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Data\n",
        "\n",
        "For these datasets, neural activity was recorded from two Utah arrays: one implanted in the dorsal premotor cortex, which is thought to play a role in movement planning, and one in the primary motor cortex. This recorded data was spike sorted offline into the provided unit spike times. In addition to the neural data, cursor position and the monkeys' hand and gaze position were recorded during the experiment, and we estimated the hand velocity offline using the recorded hand position.\n",
        "\n",
        "The MC_Maze datasets are entirely trialized, and no data was recorded between trials. As a result, though the data is presented here as a single continuous block, trials are separated by NaN margins to indicate when the data is discontinuous. In addition, in three of our dataset files, we reduced the number of trials to a fixed amount for evaluation of model performance on limited data."
      ],
      "metadata": {
        "id": "pIapt-2McRGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Exploring the data\n",
        "\n",
        "### 2.1 Setup\n",
        "\n",
        "First, let's make the necessary imports and load the dataset."
      ],
      "metadata": {
        "id": "sOHNgfyocRGw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Download dataset and required packages if necessary\n",
        "!pip install git+https://github.com/neurallatents/nlb_tools.git\n",
        "!pip install dandi\n",
        "!dandi download https://gui.dandiarchive.org/dandiset/000128"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/neurallatents/nlb_tools.git\n",
            "  Cloning https://github.com/neurallatents/nlb_tools.git to /tmp/pip-req-build-csaqeus7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neurallatents/nlb_tools.git /tmp/pip-req-build-csaqeus7\n",
            "  Resolved https://github.com/neurallatents/nlb_tools.git to commit 1ddc15f45b56388ff093d1396b7b87b36fa32a68\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas<=1.3.4,>=1.0.0 (from nlb_tools==0.0.3)\n",
            "  Downloading pandas-1.3.4.tar.gz (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from nlb_tools==0.0.3) (1.16.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from nlb_tools==0.0.3) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from nlb_tools==0.0.3) (1.6.1)\n",
            "Requirement already satisfied: h5py<4,>=2.9 in /usr/local/lib/python3.12/dist-packages (from nlb_tools==0.0.3) (3.15.1)\n",
            "Collecting pynwb (from nlb_tools==0.0.3)\n",
            "  Downloading pynwb-3.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.12/dist-packages (from pandas<=1.3.4,>=1.0.0->nlb_tools==0.0.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.12/dist-packages (from pandas<=1.3.4,>=1.0.0->nlb_tools==0.0.3) (2025.2)\n",
            "Collecting hdmf<5,>=4.1.0 (from pynwb->nlb_tools==0.0.3)\n",
            "  Downloading hdmf-4.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: platformdirs>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from pynwb->nlb_tools==0.0.3) (4.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->nlb_tools==0.0.3) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->nlb_tools==0.0.3) (3.6.0)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3) (4.25.1)\n",
            "Collecting ruamel-yaml>=0.16 (from hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3)\n",
            "  Downloading ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.3->pandas<=1.3.4,>=1.0.0->nlb_tools==0.0.3) (1.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3) (0.28.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml>=0.16->hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3)\n",
            "  Downloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb->nlb_tools==0.0.3) (4.15.0)\n",
            "Downloading pynwb-3.1.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdmf-4.1.2-py3-none-any.whl (339 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.5/339.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (753 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nlb_tools, pandas\n",
            "  Building wheel for nlb_tools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlb_tools: filename=nlb_tools-0.0.3-py3-none-any.whl size=39817 sha256=b465a440c71914672263b6aac44cab2d63ea1afa907533e57405ce73b27d51ec\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n7gfkya7/wheels/03/13/42/987990fe5e18caf2c2f3f515f8ded8c12a345f2d034df43722\n",
            "  Building wheel for pandas (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting dandi\n",
            "  Downloading dandi-0.73.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting bidsschematools~=1.0 (from dandi)\n",
            "  Downloading bidsschematools-1.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting bids-validator-deno>=2.0.5 (from dandi)\n",
            "  Downloading bids_validator_deno-2.2.2-py2.py3-none-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting click<8.2.0,>=7.1 (from dandi)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting click-didyoumean (from dandi)\n",
            "  Downloading click_didyoumean-0.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting dandischema<0.12.0,>=0.11.1 (from dandi)\n",
            "  Downloading dandischema-0.11.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting etelemetry>=0.2.2 (from dandi)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting fasteners (from dandi)\n",
            "  Downloading fasteners-0.20-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting fscacher>=0.3.0 (from dandi)\n",
            "  Downloading fscacher-0.4.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from dandi) (3.15.1)\n",
            "Collecting hdmf!=3.14.4,!=3.5.0 (from dandi)\n",
            "  Using cached hdmf-4.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from dandi) (4.14.0)\n",
            "Collecting interleave~=0.3 (from dandi)\n",
            "  Downloading interleave-0.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from dandi) (1.5.2)\n",
            "Requirement already satisfied: keyring!=23.9.0 in /usr/local/lib/python3.12/dist-packages (from dandi) (25.6.0)\n",
            "Collecting keyrings.alt (from dandi)\n",
            "  Downloading keyrings.alt-5.0.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from dandi) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from dandi) (4.5.0)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.12/dist-packages (from dandi) (3.23.0)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.12/dist-packages (from dandi) (2.11.10)\n",
            "Collecting pynwb!=1.1.0,!=2.3.0,>=1.0.3 (from dandi)\n",
            "  Using cached pynwb-3.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting numcodecs<0.16 (from dandi)\n",
            "  Downloading numcodecs-0.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting nwbinspector!=0.4.32,>=0.4.28 (from dandi)\n",
            "  Downloading nwbinspector-0.6.5-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pyout!=0.6.0,>=0.5 (from dandi)\n",
            "  Downloading pyout-0.8.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from dandi) (2.9.0.post0)\n",
            "Requirement already satisfied: requests~=2.20 in /usr/local/lib/python3.12/dist-packages (from dandi) (2.32.4)\n",
            "Collecting ruamel.yaml<1,>=0.15 (from dandi)\n",
            "  Using cached ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.12/dist-packages (from dandi) (2.10.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (from dandi) (8.5.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from dandi) (0.1.78)\n",
            "Requirement already satisfied: urllib3>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from dandi) (2.5.0)\n",
            "Requirement already satisfied: yarl~=1.9 in /usr/local/lib/python3.12/dist-packages (from dandi) (1.22.0)\n",
            "Collecting zarr<=3.0.8,>=2.10 (from dandi)\n",
            "  Downloading zarr-3.0.8-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting zarr_checksum~=0.4.0 (from dandi)\n",
            "  Downloading zarr_checksum-0.4.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting acres (from bidsschematools~=1.0->dandi)\n",
            "  Downloading acres-0.5.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from bidsschematools~=1.0->dandi) (6.0.3)\n",
            "Requirement already satisfied: jsonschema[format] in /usr/local/lib/python3.12/dist-packages (from dandischema<0.12.0,>=0.11.1->dandi) (4.25.1)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.2->dandi)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from hdmf!=3.14.4,!=3.5.0->dandi) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from hdmf!=3.14.4,!=3.5.0->dandi) (2.2.2)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring!=23.9.0->dandi) (3.4.1)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring!=23.9.0->dandi) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring!=23.9.0->dandi) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring!=23.9.0->dandi) (4.3.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring!=23.9.0->dandi) (6.0.1)\n",
            "Collecting deprecated (from numcodecs<0.16->dandi)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from nwbinspector!=0.4.32,>=0.4.28->dandi) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from nwbinspector!=0.4.32,>=0.4.28->dandi) (2025.3.0)\n",
            "Collecting hdmf-zarr (from nwbinspector!=0.4.32,>=0.4.28->dandi)\n",
            "  Downloading hdmf_zarr-0.12.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting isodate (from nwbinspector!=0.4.32,>=0.4.28->dandi)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from nwbinspector!=0.4.32,>=0.4.28->dandi) (8.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nwbinspector!=0.4.32,>=0.4.28->dandi) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dandi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dandi) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dandi) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->dandi) (0.4.2)\n",
            "Collecting blessed (from pyout!=0.6.0,>=0.5->dandi)\n",
            "  Downloading blessed-1.23.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->dandi) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.20->dandi) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.20->dandi) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.20->dandi) (2025.10.5)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml<1,>=0.15->dandi)\n",
            "  Using cached ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl~=1.9->dandi) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl~=1.9->dandi) (0.4.1)\n",
            "Collecting donfig>=0.8 (from zarr<=3.0.8,>=2.10->dandi)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorstore->dandi) (0.5.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (0.28.0)\n",
            "Collecting crc32c>=2.7 (from numcodecs[crc32c]>=0.14->zarr<=3.0.8,>=2.10->dandi)\n",
            "  Downloading crc32c-2.8-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->hdmf!=3.14.4,!=3.5.0->dandi) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->hdmf!=3.14.4,!=3.5.0->dandi) (2025.2)\n",
            "Collecting email-validator>=2.0.0 (from pydantic[email]~=2.4->dandischema<0.12.0,>=0.11.1->dandi)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring!=23.9.0->dandi) (43.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->nwbinspector!=0.4.32,>=0.4.28->dandi) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->nwbinspector!=0.4.32,>=0.4.28->dandi) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->nwbinspector!=0.4.32,>=0.4.28->dandi) (1.8.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.12/dist-packages (from blessed->pyout!=0.6.0,>=0.5->dandi) (0.2.14)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->numcodecs<0.16->dandi) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from hdmf-zarr->nwbinspector!=0.4.32,>=0.4.28->dandi) (3.6.0)\n",
            "Collecting zarr<=3.0.8,>=2.10 (from dandi)\n",
            "  Downloading zarr-2.18.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting asciitree (from zarr<=3.0.8,>=2.10->dandi)\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring!=23.9.0->dandi) (10.8.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (0.1.4)\n",
            "Collecting rfc3987 (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi)\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (25.10.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring!=23.9.0->dandi) (2.0.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]~=2.4->dandischema<0.12.0,>=0.11.1->dandi)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format]->dandischema<0.12.0,>=0.11.1->dandi) (1.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring!=23.9.0->dandi) (2.23)\n",
            "Downloading dandi-0.73.2-py3-none-any.whl (368 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.5/368.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bids_validator_deno-2.2.2-py2.py3-none-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (43.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidsschematools-1.1.2-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.1/180.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dandischema-0.11.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Downloading fscacher-0.4.4-py3-none-any.whl (12 kB)\n",
            "Using cached hdmf-4.1.2-py3-none-any.whl (339 kB)\n",
            "Downloading interleave-0.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading numcodecs-0.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nwbinspector-0.6.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pynwb-3.1.2-py3-none-any.whl (1.4 MB)\n",
            "Downloading pyout-0.8.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n",
            "Downloading zarr_checksum-0.4.7-py3-none-any.whl (15 kB)\n",
            "Downloading click_didyoumean-0.3.1-py3-none-any.whl (3.6 kB)\n",
            "Downloading fasteners-0.20-py3-none-any.whl (18 kB)\n",
            "Downloading keyrings.alt-5.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Using cached ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (753 kB)\n",
            "Downloading acres-0.5.0-py3-none-any.whl (12 kB)\n",
            "Downloading blessed-1.23.0-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading hdmf_zarr-0.12.0-py3-none-any.whl (33 kB)\n",
            "Downloading zarr-2.18.7-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.3/211.3 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: asciitree\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5031 sha256=9ba5b4fb179db8205a32e496019cf75cdde80e83036398c03c71da8018b4e45e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/d7/98/f56ae733748cd0fa577172bda0e73e0b1f1793c98e09b9e458\n",
            "Successfully built asciitree\n",
            "Installing collected packages: rfc3987, bids-validator-deno, asciitree, ruamel.yaml.clib, isodate, interleave, fscacher, fasteners, dnspython, deprecated, click, ci-info, blessed, acres, zarr_checksum, ruamel.yaml, numcodecs, keyrings.alt, etelemetry, email-validator, click-didyoumean, bidsschematools, zarr, pyout, hdmf, pynwb, dandischema, hdmf-zarr, nwbinspector, dandi\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "Successfully installed acres-0.5.0 asciitree-0.3.3 bids-validator-deno-2.2.2 bidsschematools-1.1.2 blessed-1.23.0 ci-info-0.3.0 click-8.1.8 click-didyoumean-0.3.1 dandi-0.73.2 dandischema-0.11.1 deprecated-1.3.1 dnspython-2.8.0 email-validator-2.3.0 etelemetry-0.3.1 fasteners-0.20 fscacher-0.4.4 hdmf-4.1.2 hdmf-zarr-0.12.0 interleave-0.3.0 isodate-0.7.2 keyrings.alt-5.0.2 numcodecs-0.15.1 nwbinspector-0.6.5 pynwb-3.1.2 pyout-0.8.1 rfc3987-1.3.8 ruamel.yaml-0.18.16 ruamel.yaml.clib-0.2.14 zarr-2.18.7 zarr_checksum-0.4.7\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcaeyfDccRGw",
        "outputId": "3906d9fc-3c1e-42de-ae92-b156da763c4a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74cd6bf9"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, message='.*unverified HTTPS request.*')\n",
        "!pip install pandas==2.2.2 --force-reinstall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Imports\n",
        "\n",
        "# %matplotlib widget # uncomment for interactive plots\n",
        "from nlb_tools.nwb_interface import NWBDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "outputs": [],
      "metadata": {
        "id": "91Bb2uZWcRGx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28a9600"
      },
      "source": [
        "After running the above cell, please re-run the 'Imports' cell (cell `91Bb2uZWcRGx`) to ensure all libraries are loaded with the correct `pandas` version. You might need to restart the runtime and run all cells up to that point if the issue persists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Load dataset\n",
        "dataset = NWBDataset(\"/content/000128/sub-Jenkins/\", \"*train\", split_heldout=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KKUBssj0cRGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepping the dataset\n",
        "dataset_copy = dataset.data.copy()\n",
        "df = dataset_copy.reset_index()\n",
        "times_trials = dataset.trial_info\n",
        "\n",
        "#converting times into milliseconds\n",
        "df[\"milliseconds\"]=df[\"clock_time\"].dt.total_seconds()*1000\n",
        "times_trials[\"start_ms\"]=times_trials[\"start_time\"].dt.total_seconds()*1000\n",
        "times_trials[\"end_ms\"]=times_trials[\"end_time\"].dt.total_seconds()*1000"
      ],
      "metadata": {
        "id": "5Hqk-3Nocu4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mapping each millisecond to the trial index based on the trial start & end times\n",
        "df['corresponding_trial'] = -1\n",
        "\n",
        "for idx, row in times_trials.iterrows():\n",
        "    mask = (df['milliseconds'] >= row['start_ms']) & (df['milliseconds'] <= row['end_ms'])\n",
        "    df.loc[mask, 'corresponding_trial'] = row['trial_id']"
      ],
      "metadata": {
        "id": "9nV2alOZcu1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neurons = df[\"spikes\"].columns.to_list() #list of recorded neurons\n",
        "n_dict = {} #if a neuron fired at all during a particular trial, how many times?\n",
        "for N in neurons:\n",
        "    neuron_trials = df[df[(\"spikes\", N)] > 0.0][\"corresponding_trial\"].unique()\n",
        "    n_dict[N]=neuron_trials"
      ],
      "metadata": {
        "id": "PAd7DMaYcuy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a padding function to make the lists equal in length in order to create a df\n",
        "def pad_lists(list_dict):\n",
        "    max_len = max(len(v) for v in list_dict.values())\n",
        "    padded = {\n",
        "        k: list(v) + [np.nan] * (max_len - len(v))   # convert to list first\n",
        "        for k, v in list_dict.items()\n",
        "    }\n",
        "    return pd.DataFrame(padded)\n",
        "n_df = pad_lists(n_dict)\n",
        "\n",
        "#calculating the correlation matrix (for each pair of recorded neurons,\n",
        "#what was the correlation in terms of their trial-based firing?)\n",
        "corr = n_df.corr()\n",
        "print(corr)"
      ],
      "metadata": {
        "id": "lorgVKnPcuxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#possible reason for result: lots of zeros in the matrix\n",
        "#strategy shift - instead of focusing on trial-based firing correspondance,\n",
        "#look at the more granular level of ms, i.e., what neurons seem to consistently\n",
        "#fire together across different milliseconds?"
      ],
      "metadata": {
        "id": "3w4yc4bTcuuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a spike train array for each neuron, i.e., a simple array of the neuron's firing rate at each millisecond\n",
        "spikes = {nid: df[(\"spikes\", nid)].values.astype(np.uint8)\n",
        "          for nid in neurons}"
      ],
      "metadata": {
        "id": "D5iHL9tYcusJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a cross-correlogram function: how do spikes from one neuron relate to\n",
        "#spikes from another neuron across the time period - useful for functional connections\n",
        "\n",
        "#how does it work:\n",
        "#step 1: expand the spike count array to a spike-time array\n",
        "#step 2: determine the lags: we chose the -80ms to +80ms lag in line with what the paper got for the best results\n",
        "#step 3: for each spike of neuron a, calculate the difference in spike times with each spike of neuron b and map to one of the lag \"keys\"\n",
        "#step 4: how many of these (a, b) spike pairs fall into each lag category\n",
        "def ccg_count(a, b, max_lag=80):\n",
        "    spk_a = np.repeat(np.arange(len(a)), a)\n",
        "    spk_b = np.repeat(np.arange(len(b)), b)\n",
        "\n",
        "    cc = np.zeros(2*max_lag+1, dtype=np.int32)\n",
        "\n",
        "    for t in spk_a:\n",
        "        diffs = spk_b - t\n",
        "        valid = diffs[(diffs >= -max_lag) & (diffs <= max_lag)]\n",
        "        np.add.at(cc, valid + max_lag, 1)\n",
        "\n",
        "    lags = np.arange(-max_lag, max_lag+1)\n",
        "    return lags, cc"
      ],
      "metadata": {
        "id": "9SYYy0A6cuoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#jittering each spike time randomly as a precaution against false correlation\n",
        "#how does it work?\n",
        "#imagine my average time to compute to campus is 20 minutes and classmate who lives on campus is 10 minutes. both of us have class at the same time in WLH\n",
        "# this is a true correlation cuz we are systematically arriving at WLH across times (today, tomorrow, day after). lets say i bump into a friend whi happens to be p\n",
        "# passing from in front of WLH as I arrive at campus. they are not taking the class but they happened to be at wlh the same time as me (false correlation)\n",
        "#jittering will shift our times - i arrive say 10 minutes late (at t=30) to campus, my classmate arrives 5 minutes early (t=5). the friendi bumped into is also\n",
        "#jittered and arrives 7 minutes early. In this case, me and my classmate are still at wlh during the \"event\" (class) but the friend and I no longer\n",
        "#bump into each other (false correlation)\n",
        "def jitter_spikes_count(train, window=20):\n",
        "    spk_times = np.repeat(np.arange(len(train)), train)  # all spike times\n",
        "    jit = np.zeros_like(train)\n",
        "\n",
        "    for t in spk_times:\n",
        "        t_new = t + np.random.randint(-window, window+1)\n",
        "        if 0 <= t_new < len(train):\n",
        "            jit[t_new] += 1  # maintain multiple spikes\n",
        "    return jit\n",
        "\n",
        "  #applying jittering to the ccg output\n",
        "def jitter_corrected_ccg(a, b, max_lag=80, window=20, n_shuffles=50):\n",
        "  lags, raw = ccg_count(a, b, max_lag)\n",
        "\n",
        "  baseline = np.zeros_like(raw, dtype=float)\n",
        "  for _ in range(n_shuffles):\n",
        "      aj = jitter_spikes_count(a, window)\n",
        "      bj = jitter_spikes_count(b, window)\n",
        "      _, sh = ccg_count(aj, bj, max_lag)\n",
        "      baseline += sh\n",
        "\n",
        "  baseline /= n_shuffles\n",
        "  return lags, raw - baseline"
      ],
      "metadata": {
        "id": "LmuFj6NZculU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assigns a score to the neuron a and b's functional connectivity based on systematic relation of spike times\n",
        "def compute_connectivity(spikes, max_lag=80):\n",
        "    ids = list(spikes.keys())\n",
        "    N = len(ids)\n",
        "    conn = np.zeros((N, N))\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            lags, corr = jitter_corrected_ccg(spikes[ids[i]], spikes[ids[j]], max_lag)\n",
        "\n",
        "            # connectivity metric: mean from lag 1 to 6 ms\n",
        "            c = corr[max_lag+1 : max_lag+6].mean()\n",
        "            conn[i, j] = c\n",
        "\n",
        "    return ids, conn"
      ],
      "metadata": {
        "id": "CpwuPTa1cuRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx"
      ],
      "metadata": {
        "id": "hL26vhrqpi7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "ids, conn = compute_connectivity(spikes)\n",
        "\n",
        "# convert to graph (only keep significant or positive edges)\n",
        "G = nx.from_numpy_array(conn > 0)    # threshold optional\n",
        "\n",
        "# relabel nodes from indices to neuron IDs\n",
        "mapping = {i: ids[i] for i in range(len(ids))}\n",
        "G = nx.relabel_nodes(G, mapping)\n",
        "\n",
        "# community detection (Louvain-style)\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "communities = greedy_modularity_communities(G)\n",
        "\n",
        "for i, community in enumerate(communities):\n",
        "    print(f\"Community {i+1}: {sorted(community)}\")\n",
        "conn_df = pd.DataFrame(conn, index=ids, columns=ids)\n",
        "print(conn_df.head())\n"
      ],
      "metadata": {
        "id": "Ya-Rwv-FptEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = nx.spring_layout(G, seed=42)\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan', 'magenta']\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "for i, community in enumerate(communities):\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=community,\n",
        "                           node_color=colors[i % len(colors)], node_size=500)\n",
        "nx.draw_networkx_edges(G, pos)\n",
        "nx.draw_networkx_labels(G, pos)\n",
        "plt.title(\"Neuron Communities\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1opRIYiwp04B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Continuous data\n",
        "\n",
        "The continuous data provided with the MC_Maze datasets includes:\n",
        "* `cursor_pos` - x and y position of the cursor controlled by the monkey\n",
        "* `eye_pos` - x and y position of the monkey's point of gaze on the screen, in mm\n",
        "* `hand_pos` - x and y position of the monkey's hand, in mm\n",
        "* `hand_vel` - x and y velocities of the monkey's hand, in mm/s, computed offline using `np.gradient`\n",
        "* `spikes` - spike times binned at 1 ms"
      ],
      "metadata": {
        "id": "wTUi0cmhcRGy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## View 'dataset.data'\n",
        "dataset.data"
      ],
      "outputs": [],
      "metadata": {
        "id": "EZjdwb86cRGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Trial metadata\n",
        "\n",
        "The trial info dataframe has a number of fields containing information about each trial:\n",
        "* `trial_id` - a number assigned to each trial during loading\n",
        "* `start_time` - time when the trial begins\n",
        "* `end_time` - time when the trial ends\n",
        "* `trial_type` - the maze configuration that was used for the trial\n",
        "* `trial_version` - a number 0-2 indicating which variant of the maze is presented. 0 is 1-target no-barrier, 1 is 1-target with barriers, 2 is 3-target with barriers\n",
        "* `maze_id` - a unique identifier for the maze configuration used. Different maze sets were used for each session, so `trial_type` is not unique across dataset files\n",
        "* `success` - whether the trial was successful. In provided training data, unsuccessful trials have already been removed\n",
        "* `target_on_time` - time of target presentation\n",
        "* `go_cue_time` - time of go cue\n",
        "* `move_onset_time` - time of movement onset, calculated offline with robust algorithm\n",
        "* `rt` - reaction time in ms\n",
        "* `delay` - time between target presentation and go cue in ms\n",
        "* `num_targets` - number of targets displayed in the maze\n",
        "* `target_pos` - x and y position of the target(s)\n",
        "* `num_barriers` - number of barriers in the maze\n",
        "* `barrier_pos` - position of the barrier(s). First two values are the x and y positions of the center of the barrier, last two values are the half-width and half-height of the barrier\n",
        "* `active_target` - which target is reachable and was hit by the monkey. Its value corresponds to the index of the target in `target_pos`"
      ],
      "metadata": {
        "id": "8Zt8QmVjcRGy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## View 'dataset.trial_info'\n",
        "dataset.trial_info"
      ],
      "outputs": [],
      "metadata": {
        "id": "zfdwRuARcRGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Reach conditions\n",
        "\n",
        "The full MC_Maze dataset has 108 different reach conditions, and the reduced-size datasets each have 27. Because of the maze barriers, reaches take on a variety of straight and curved trajectories. Here, we'll plot the average trajectory per condition to see what typical reaches look like."
      ],
      "metadata": {
        "id": "zsEEQjrxcRGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Optional resampling\n",
        "# It may be beneficial to resample the data before you proceed to the analysis sections,\n",
        "# as they may be fairly memory-intensive. However, we have not tested this notebook at bin sizes\n",
        "# of over 20 ms, so we cannot guarantee that everything will work as intended at\n",
        "# those larger bin sizes. If you still have memory issues, you could also use the reduced size\n",
        "# datasets instead of the full one.\n",
        "dataset.resample(5)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RHJ7cJaTcRGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Plot trial-averaged reaches\n",
        "\n",
        "# Find unique conditions\n",
        "conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
        "\n",
        "# Initialize plot\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
        "\n",
        "# Loop over conditions and compute average trajectory\n",
        "for cond in conds:\n",
        "    # Find trials in condition\n",
        "    mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "    # Extract trial data\n",
        "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
        "    # Average hand position across trials\n",
        "    traj = trial_data.groupby('align_time')[[('hand_pos', 'x'), ('hand_pos', 'y')]].mean().to_numpy()\n",
        "    # Determine reach angle for color\n",
        "    active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
        "    reach_angle = np.arctan2(*active_target[::-1])\n",
        "    # Plot reach\n",
        "    ax.plot(traj[:, 0], traj[:, 1], linewidth=0.7, color=plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Cp2A-Y9ycRG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Single-neuron responses\n",
        "\n",
        "As shown above, there are a large number of conditions in the full MC_Maze dataset which differ in the position and number of targets and barriers. While many of the reaches follow similar trajectories, the differences in the exact positions of targets and barriers may result in slight differences in observed neural activity, especially during pre-movement planning periods. Here, we'll plot PSTHs for a single neuron for a subset of the conditions."
      ],
      "metadata": {
        "id": "CXv8433NcRG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Plot PSTHs\n",
        "\n",
        "# Seed generator for consistent plots\n",
        "np.random.seed(2468)\n",
        "n_conds = 8 # number of conditions to plot\n",
        "\n",
        "# Smooth spikes with 50 ms std Gaussian\n",
        "dataset.smooth_spk(50, name='smth_50')\n",
        "\n",
        "# Plot random neuron\n",
        "neur_num = np.random.choice(dataset.data.spikes.columns)\n",
        "\n",
        "# Find unique conditions\n",
        "conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
        "\n",
        "# Plot random subset of conditions\n",
        "for i in np.random.choice(len(conds), size=n_conds, replace=False):\n",
        "    cond = conds[i]\n",
        "    # Find trials in condition\n",
        "    mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "    # Extract trial data\n",
        "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
        "    # Average hand position across trials\n",
        "    psth = trial_data.groupby('align_time')[[('spikes_smth_50', neur_num)]].mean().to_numpy() / dataset.bin_width * 1000\n",
        "    # Color PSTHs by reach angle\n",
        "    active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
        "    reach_angle = np.arctan2(*active_target[::-1])\n",
        "    # Plot reach\n",
        "    plt.plot(np.arange(-50, 450, dataset.bin_width), psth, label=cond, color=plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
        "\n",
        "# Add labels\n",
        "plt.ylim(bottom=0)\n",
        "plt.xlabel('Time after movement onset (ms)')\n",
        "plt.ylabel('Firing rate (spk/s)')\n",
        "plt.title(f'Neur {neur_num} PSTH')\n",
        "plt.legend(title='condition', loc='upper right')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zl52f2j6cRG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see,  this neuron displays a variety of responses in different conditions. However, the information provided by this individual neuron is limited, and this method relies on averaging across trials, which discards single-trial variablity as noise. Instead of averaging across trials, we can use the activity of the whole neural population on a single-trial basis to extract behaviorally-relevant information."
      ],
      "metadata": {
        "id": "adiRbFzocRG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Decoding hand kinematics\n",
        "\n",
        "Next, we'll try to decode the monkey's hand velocity solely from smoothed population spiking activity. Since it takes time for signals to travel from the motor cortex to muscles, we lag the kinematics data relative to neural data. 80 ms lag is where we have seen the best results, but feel free to vary the value and compare performance."
      ],
      "metadata": {
        "id": "hej-DpkPcRG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Kinematic decoding\n",
        "\n",
        "# Extract neural data and lagged hand velocity\n",
        "trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-130, 370))\n",
        "lagged_trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450))\n",
        "rates = trial_data.spikes_smth_50.to_numpy()\n",
        "vel = lagged_trial_data.hand_vel.to_numpy()\n",
        "\n",
        "# Fit and evaluate decoder\n",
        "gscv = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 0, 5)})\n",
        "gscv.fit(rates, vel)\n",
        "pred_vel = gscv.predict(rates)\n",
        "print(f\"Decoding R2: {gscv.best_score_}\")\n",
        "\n",
        "# Merge predictions back to continuous data\n",
        "pred_vel_df = pd.DataFrame(pred_vel, index=lagged_trial_data.clock_time, columns=pd.MultiIndex.from_tuples([('pred_vel', 'x'), ('pred_vel', 'y')]))\n",
        "dataset.data = pd.concat([dataset.data, pred_vel_df], axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "TWk7xRiscRG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got an R2 of around 0.6, which is not too bad. Let's visualize how our predicted kinematics compare to the true data. We'll plot results for only one condition to keep things from getting too cluttered."
      ],
      "metadata": {
        "id": "6Uc-oOqTcRG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Plot predicted vs true kinematics\n",
        "\n",
        "# Choose 23rd condition to plot\n",
        "cond = conds[23]\n",
        "\n",
        "# Find trials in condition and extract data\n",
        "mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
        "\n",
        "# Initialize figure\n",
        "fig, axs = plt.subplots(2, 3, figsize=(10, 4))\n",
        "t = np.arange(-50, 450, dataset.bin_width)\n",
        "\n",
        "# Loop through trials in condition\n",
        "for _, trial in trial_data.groupby('trial_id'):\n",
        "    # True and predicted x velocity\n",
        "    axs[0][0].plot(t, trial.hand_vel.x, linewidth=0.7, color='black')\n",
        "    axs[1][0].plot(t, trial.pred_vel.x, linewidth=0.7, color='blue')\n",
        "    # True and predicted y velocity\n",
        "    axs[0][1].plot(t, trial.hand_vel.y, linewidth=0.7, color='black')\n",
        "    axs[1][1].plot(t, trial.pred_vel.y, linewidth=0.7, color='blue')\n",
        "    # True and predicted trajectories\n",
        "    true_traj = np.cumsum(trial.hand_vel.to_numpy(), axis=0) * dataset.bin_width / 1000\n",
        "    pred_traj = np.cumsum(trial.pred_vel.to_numpy(), axis=0) * dataset.bin_width / 1000\n",
        "    axs[0][2].plot(true_traj[:, 0], true_traj[:, 1], linewidth=0.7, color='black')\n",
        "    axs[1][2].plot(pred_traj[:, 0], pred_traj[:, 1], linewidth=0.7, color='blue')\n",
        "\n",
        "# Set up shared axes\n",
        "for i in range(2):\n",
        "    axs[i][0].set_xlim(-50, 450)\n",
        "    axs[i][1].set_xlim(-50, 450)\n",
        "    axs[i][2].set_xlim(-180, 180)\n",
        "    axs[i][2].set_ylim(-130, 130)\n",
        "\n",
        "# Add labels\n",
        "axs[0][0].set_title('X velocity (mm/s)')\n",
        "axs[0][1].set_title('Y velocity (mm/s)')\n",
        "axs[0][2].set_title('Reach trajectory')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "tags": [],
        "id": "TDXGlC_ZcRG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, decoding from single-trial smoothed spikes is able to recover general trends in hand kinematics but cannot accurately recover the true kinematics very well, particularly in the y-direction."
      ],
      "metadata": {
        "id": "GWVI4RvucRG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 Neural trajectories\n",
        "\n",
        "Finally, we'll look at how neural population activity evolves over time in each condition by applying PCA to trial-averaged smoothed spikes. We'll plot the resulting trajectories and compare them across conditions."
      ],
      "metadata": {
        "id": "yeYTIoyzcRG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Plot neural trajectories for subset of conditions\n",
        "\n",
        "# Seed generator for consistent plots\n",
        "np.random.seed(2021)\n",
        "n_conds = 27 # number of conditions to plot\n",
        "\n",
        "# Get unique conditions\n",
        "conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
        "\n",
        "# Loop through conditions\n",
        "rates = []\n",
        "colors = []\n",
        "for i in np.random.choice(len(conds), n_conds):\n",
        "    cond = conds[i]\n",
        "    # Find trials in condition\n",
        "    mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "    # Extract trial data\n",
        "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
        "    # Append averaged smoothed spikes for condition\n",
        "    rates.append(trial_data.groupby('align_time')[trial_data[['spikes_smth_50']].columns].mean().to_numpy())\n",
        "    # Append reach angle-based color for condition\n",
        "    active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
        "    reach_angle = np.arctan2(*active_target[::-1])\n",
        "    colors.append(plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
        "\n",
        "# Stack data and apply PCA\n",
        "rate_stack = np.vstack(rates)\n",
        "rate_scaled = StandardScaler().fit_transform(rate_stack)\n",
        "pca = PCA(n_components=3)\n",
        "traj_stack = pca.fit_transform(rate_scaled)\n",
        "traj_arr = traj_stack.reshape((n_conds, len(rates[0]), -1))\n",
        "\n",
        "# Loop through trajectories and plot\n",
        "fig = plt.figure(figsize=(9, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for traj, col in zip(traj_arr, colors):\n",
        "    ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=col)\n",
        "    ax.scatter(traj[0, 0], traj[0, 1], traj[0, 2], color=col)\n",
        "\n",
        "# Add labels\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "dnNILNQfcRG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might see that conditions with similar reach angles tend to be clustered together. Note that coloring by angle to the target does not take into account the curved paths of the reaches toward the target, so you should not expect all similarly colored conditions to have similar neural trajectories.\n",
        "\n",
        "Using an alternate dimensionality reduction method called jPCA, researchers have revealed rotational dynamics in neural population activity during these maze reaches. You can read more about that in this [paper](https://www.nature.com/articles/nature11129)."
      ],
      "metadata": {
        "id": "9z7Q4k8kcRG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Summary\n",
        "\n",
        "In this notebook, we:\n",
        "* introduced the MC_Maze dataset, describing the task and provided data\n",
        "* looked at what specific continuous and trial data is included\n",
        "* demonstrated the task by plotting average reach trajectories for each condition\n",
        "* explored single-neuron responses by plotting PSTHs for some conditions\n",
        "* evaluated how accurately spike-smoothed population activity can decode hand velocity\n",
        "* visualized the timecourse of neural population activity by extracting trial-averaged neural trajectories with PCA\n",
        "\n",
        "The Maze datasets are exceptional in their combination of behavioral richness (number of task configurations), stereotyped behavior across repeated trials (tens of repeats for each task configuration), and high total trial counts (thousands). Due to the instructed delay paradigm and lack of unpredictable task events, population activity can be well-modeled as an autonomous dynamical system. Because of this, the dataset has been used for validating a number of latent variable models."
      ],
      "metadata": {
        "id": "i5lwxRLBcRG3"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "metadata": {
      "interpreter": {
        "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}